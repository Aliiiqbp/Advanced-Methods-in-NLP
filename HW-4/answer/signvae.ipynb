{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In the code box below, we first implemented a VAE. Then I fixed some bugs w.r.t dimensions and so on. Then, we found yoyo's implementation of VAE is better than all of us, and we all switched to her branch to continue her work."
      ],
      "metadata": {
        "id": "lA5DN_lub4ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First try"
      ],
      "metadata": {
        "id": "YnX2YBipc13q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH_IvplrbGFa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic VAE with convolutional encoder and deconvolutional decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        \"\"\"\n",
        "        Initializes the layers of the VAE, which should include:\n",
        "        - one dropout layer\n",
        "        - a stack of convolutional layers (we recommend starting\n",
        "          with 3 of them) interleaved with max pooling layers\n",
        "        - a dense layer to project the output from the final\n",
        "          convolution down to size self.z_dim\n",
        "        - a dense layer to project the encoder output onto mu\n",
        "        - a dense layer to project the encoder output onto sigma\n",
        "        - a stack of deconvolutional layers (AKA transposed convolutional\n",
        "          layers; we recommend starting with 4 of them) interleaved with\n",
        "          2d batch normalization layers.\n",
        "\n",
        "        Input:\n",
        "        - z_dim:    size of the codes produced by this encoder\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # TODO Your code goes here.\n",
        "        # Define the layers for the encoder\n",
        "        self.encoder_conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.encoder_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.encoder_conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.encoder_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.encoder_conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.encoder_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.encoder_fc1 = nn.Linear(64 * 8 * 8, 256) # nn.Linear(64 * 4 * 4, 256)\n",
        "        self.encoder_fc2_mu = nn.Linear(256, z_dim)\n",
        "        self.encoder_fc2_logvar = nn.Linear(256, z_dim)\n",
        "\n",
        "        # Define the layers for the decoder\n",
        "        self.decoder_fc1 = nn.Linear(z_dim, 256)\n",
        "        self.decoder_fc2 = nn.Linear(256, 64 * 8 * 8)\n",
        "\n",
        "        self.decoder_deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder_bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.decoder_deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder_bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.decoder_deconv3 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self,x):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - A torch.Tensor of size (seq_length, self.z_dim) which defaults to (50, 16)\n",
        "        \"\"\"\n",
        "        return torch.rand(x.shape[0], self.z_dim, requires_grad=True).to(x) # TODO Your code goes here.\n",
        "        # return torch.rand(x.shape[0], self.z_dim, requires_grad=True).to(x)\n",
        "\n",
        "        # TODO Your code goes here.\n",
        "        # Apply the stack of convolutional layers\n",
        "        x = F.relu(self.encoder_conv1(x))\n",
        "        x = self.encoder_pool1(x)\n",
        "        x = F.relu(self.encoder_conv2(x))\n",
        "        x = self.encoder_pool2(x)\n",
        "        x = F.relu(self.encoder_conv3(x))\n",
        "        x = self.encoder_pool3(x)\n",
        "\n",
        "        # Flatten the convolved outputs\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Project each output to size self.z_dim\n",
        "\n",
        "        x = F.relu(self.encoder_fc1(x))\n",
        "        mu = self.encoder_fc2_mu(x)         # Projection for mu\n",
        "        logvar = self.encoder_fc2_logvar(x) # Projection for log variance\n",
        "\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "    def project(self, x):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "        - A tuple of two torch.Tensors, each of shape (seq_length, self.z_dim)\n",
        "        \"\"\"\n",
        "        return (x, x) # TODO Your code goes here.\n",
        "        # return (x, x)\n",
        "        # TODO Your code goes here.\n",
        "        mu, logvar = x\n",
        "\n",
        "        # print(\"mu.shape\", mu.shape)\n",
        "        # print(\"logvar.shape\", logvar.shape)\n",
        "        # print(\"x.shape\", type(x))\n",
        "        mu = self.encoder_fc2_mu(mu)\n",
        "        logvar = self.encoder_fc2_logvar(logvar)\n",
        "\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparametrize(self, mu, sigma):\n",
        "        \"\"\"\n",
        "        - A sequence of codes z of shape (seq_length, self.z_dim) obtained by\n",
        "          sampling from a normal distribution parameterized by mu and sigma\n",
        "        \"\"\"\n",
        "        return mu + sigma # TODO Your code goes here.\n",
        "        # return mu + sigma\n",
        "        # TODO Your code goes here.\n",
        "        eps = torch.randn_like(sigma)\n",
        "        z = mu + sigma * eps\n",
        "\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        - A sequence of images of shape (seq_length, n_channels, img_width, img_height)\n",
        "          which defaults to (50, 1, 64, 64). All outputs should be in the range [0, 1].\n",
        "        \"\"\"\n",
        "        return z.repeat_interleave(4, dim=1).unsqueeze(1).repeat(1,64,1).unsqueeze(1) # TODO Your code goes here.\n",
        "        # return z.repeat_interleave(4, dim=1).unsqueeze(1).repeat(1,64,1).unsqueeze(1)\n",
        "        # TODO Your code goes here.\n",
        "        # Project the latent codes to the initial dense layer\n",
        "        x = F.relu(self.decoder_fc1(z))\n",
        "\n",
        "        # Project the result to the next dense layer to match the size of the deconvolutional layers\n",
        "        x = F.relu(self.decoder_fc2(x))\n",
        "        x = x.view(-1, 64, 4, 4)  # Reshape the output to match the size of the first deconvolutional layer\n",
        "\n",
        "        # Apply the stack of deconvolutional layers\n",
        "        x = F.relu(self.decoder_deconv1(x))\n",
        "        x = self.decoder_bn1(x)\n",
        "        x = F.relu(self.decoder_deconv2(x))\n",
        "        x = self.decoder_bn2(x)\n",
        "        x = torch.sigmoid(self.decoder_deconv3(x))  # Apply sigmoid activation to squash the output to [0, 1]\n",
        "\n",
        "        return x\n",
        "\n",
        "def kld(mu, log_var):\n",
        "    \"\"\"\n",
        "                = log(1 / sigma) + (sigma^2 + mu^2)/2 - 1/2\n",
        "                = -0.5*(1 + log(sigma^2) - sigma^2 - mu^2)\n",
        "    \"\"\"\n",
        "    return (mu + log_var).sum() # TODO Your code goes here.\n",
        "    # return (mu + log_var).sum()\n",
        "    # TODO Your code goes here.\n",
        "    kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return kld_loss\n",
        "\n",
        "def vae_loss(gen_images, input_images, mu_sigmas):\n",
        "    \"\"\"\n",
        "    KLDs = []\n",
        "\n",
        "    # TODO Your code goes here.\n",
        "    for gen_image, input_image, (mu, sigma) in zip(gen_images, input_images, mu_sigmas):\n",
        "        # Compute binary cross-entropy reconstruction loss\n",
        "        bce_loss = F.binary_cross_entropy(gen_image, input_image, reduction='sum')\n",
        "        BCEs.append(bce_loss)\n",
        "\n",
        "        # Compute KL divergence loss\n",
        "        kld_loss = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\n",
        "        KLDs.append(kld_loss)\n",
        "\n",
        "    return BCEs, KLDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Try"
      ],
      "metadata": {
        "id": "45kXsJv7c4n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic VAE with convolutional encoder and deconvolutional decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim):\n",
        "        \"\"\"\n",
        "        Initializes the layers of the VAE, which should include:\n",
        "        - 1. one dropout layer\n",
        "        - 2. a stack of convolutional layers (we recommend starting\n",
        "          with 3 of them) interleaved with max pooling layers\n",
        "        - 3. a dense layer to project the output from the final\n",
        "          convolution down to size self.z_dim\n",
        "        - 4. a dense layer to project the encoder output onto mu\n",
        "        - 5. a dense layer to project the encoder output onto sigma\n",
        "        - 6. a stack of deconvolutional layers (AKA transposed convolutional\n",
        "          layers; we recommend starting with 4 of them) interleaved with\n",
        "          2d batch normalization layers.\n",
        "\n",
        "        Input:\n",
        "        - z_dim:    size of the codes produced by this encoder\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # TODO 1.Dropout Layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # TODO 2.Encoder Layers\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride = 2)\n",
        "        )\n",
        "\n",
        "        # TODO 3. Dense Layers to project the output from the final convolution down to size self.z_dim\n",
        "        self.fc = nn.Linear(128*8*8, self.z_dim)\n",
        "\n",
        "        # TODO 4. Dense layer to project the encoder output onto mu\n",
        "        self.fc_mu = nn.Linear(self.z_dim, self.z_dim)\n",
        "\n",
        "        # TODO 5. Dense layer to project the encoder output onto sigma\n",
        "        self.fc_sigma = nn.Sequential(\n",
        "            nn.Linear(self.z_dim, self.z_dim)\n",
        "            #nn.Softplus()\n",
        "        )\n",
        "\n",
        "        # TODO 6. Decoder Layers\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=128, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Adjust the kernel size and padding here to get [50, 1, 64, 64]\n",
        "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=4, stride=2, padding=1, output_padding=0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encode(x)\n",
        "        #print('1')\n",
        "        mu_sigmas = self.project(z)\n",
        "        mu, sigma = mu_sigmas\n",
        "        #print(\"mu\", mu.size)\n",
        "        #print(\"sigma\", sigma.size)\n",
        "        z = self.reparametrize(mu, sigma)\n",
        "        z = z.unsqueeze(-1).unsqueeze(-1)\n",
        "        gen_images = self.decode(z)\n",
        "        return gen_images, mu_sigmas\n",
        "\n",
        "\n",
        "    def encode(self,x):\n",
        "        \"\"\"\n",
        "        Given a sequence of images, applies a stack of convolutional layers to\n",
        "        each image, flattens the convolved outputs, and projects each one to\n",
        "        size self.z_dim.\n",
        "\n",
        "        Input:\n",
        "        - x:    torch.Tensor of shape (seq_length, n_channels, img_width, img_height)\n",
        "                which equals (50, 1, 64, 64) with the default model configuration.\n",
        "\n",
        "        Returns:\n",
        "        - A torch.Tensor of size (seq_length, self.z_dim) which defaults to (50, 16)\n",
        "        \"\"\"\n",
        "        #seq_length = x.shape[0]\n",
        "        #n_channels = x.shape[1]\n",
        "\n",
        "        # update the in_channels in the 1st lay of encoder\n",
        "        #self.encoder[0] = nn.Conv2d(in_channels=n_channels, out_channels = 16, kernel_size=3, stride = 1, padding = 1)\n",
        "\n",
        "        #y = self.encoder(x)\n",
        "        #y = y.view(seq_length, -1)\n",
        "\n",
        "        #placeholder = torch.rand(seq_length, self.z_dim, requires_grad = True).to(x)\n",
        "        #assert placeholder.size() == y.size(), \"encoder error: size not match\"\n",
        "\n",
        "        #print(x.shape)\n",
        "        y  = self.encoder(x)\n",
        "        #print(y.shape)\n",
        "        y = y.view(50, -1)\n",
        "        #print(y.shape)\n",
        "        y  = self.fc(y)\n",
        "        #print(y.shape)\n",
        "        return y\n",
        "\n",
        "\n",
        "    def project(self, x):\n",
        "        \"\"\"\n",
        "        Given an intermediate sequence of encoded images, applies two\n",
        "        projections to each encoding to produce vectors mu and sigma.\n",
        "\n",
        "        Input:\n",
        "        - x:    torch.Tensor of shape (seq_length, self.z_dim) (output\n",
        "                from self.encode)\n",
        "\n",
        "        Returns:\n",
        "        - A tuple of two torch.Tensors, each of shape (seq_length, self.z_dim)\n",
        "        \"\"\"\n",
        "        seq_length = x.shape[0]\n",
        "\n",
        "        mu = self.fc_mu(x)\n",
        "        sigma = self.fc_sigma(x)\n",
        "\n",
        "        assert x.size() == mu.size(), \"project error: mean size not match\"\n",
        "        assert x.size() == sigma.size(), \"project error: val size not match\"\n",
        "\n",
        "        return (mu, sigma)\n",
        "\n",
        "    def reparametrize(self, mu, sigma):\n",
        "        \"\"\"\n",
        "        Applies the reparametrization trick from\n",
        "        https://arxiv.org/pdf/1312.6114v10.pdf\n",
        "\n",
        "        Input:\n",
        "        - mu:       torch.Tensor of shape (seq_length, self.z_dim) returned\n",
        "                    by self.project()\n",
        "        - sigma:    torch.Tensor of shape (seq_length, self.z_dim) returned\n",
        "                    by self.project()\n",
        "\n",
        "        Returns:\n",
        "        - A sequence of codes z of shape (seq_length, self.z_dim) obtained by\n",
        "          sampling from a normal distribution parameterized by mu and sigma\n",
        "        \"\"\"\n",
        "        epislon = torch.randn_like(sigma)\n",
        "\n",
        "        z = mu + sigma * epislon\n",
        "\n",
        "        placeholder = mu + sigma\n",
        "\n",
        "        assert placeholder.size() == z.size(), \"reparameter error: size not match\"\n",
        "\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Given a sequence of variational codes, applies a stack of deconvolutional\n",
        "        layers (AKA transposed convolutional layers) to recover a sequence of images.\n",
        "\n",
        "        Input:\n",
        "        - z:    torch.Tensor of shape (seq_length, self.z_dim) returned by\n",
        "                self.reparametrize()\n",
        "\n",
        "        Returns:\n",
        "        - A sequence of images of shape (seq_length, n_channels, img_width, img_height)\n",
        "          which defaults to (50, 1, 64, 64). All outputs should be in the range [0, 1].\n",
        "        \"\"\"\n",
        "        #print('lets decode')\n",
        "        #print(z.shape)\n",
        "        seq_length = z.shape[0]\n",
        "        assert seq_length == 50, \"decode error: size of seq_length not match to default\"\n",
        "        n_channels = 1\n",
        "        img_width = 64\n",
        "        img_height = 64\n",
        "\n",
        "       # self.decoder[-2] = nn.ConvTranspose2d(in_channels=64, out_channels=n_channels, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        z = z.unsqueeze(-1).unsqueeze(-1)\n",
        "        #print(z.shape)\n",
        "        y = self.decoder(z)\n",
        "        #print(y.shape)\n",
        "\n",
        "        #placeholder = z.repeat_interleave(4, dim=1).unsqueeze(1).repeat(1,64,1).unsqueeze(1)\n",
        "        #assert placeholder.size() == y.size(), \"decode error: size not match\"\n",
        "\n",
        "        return y\n",
        "\n",
        "def kld(mu, log_var):\n",
        "    \"\"\"\n",
        "    Computes KL div loss wrt. a standard normal prior.\n",
        "\n",
        "    Input:\n",
        "    - log_var:  log variance of encoder outputs\n",
        "    - mu:       mean of encoder outputs\n",
        "\n",
        "    Returns:    D_{KL}(\\mathcal{N}(mu, sigma) || \\mathcal{N}(0, 1))\n",
        "                = log(1 / sigma) + (sigma^2 + mu^2)/2 - 1/2\n",
        "                = -0.5*(1 + log(sigma^2) - sigma^2 - mu^2)\n",
        "    \"\"\"\n",
        "    #print(\"log var\", log_var)\n",
        "    log_var = torch.clamp(log_var, min = -0.2, max = 0.2)\n",
        "    #print(\"log var after clamp\", log_var)\n",
        "    kld_loss = -0.5 * torch.sum( 1+log_var -log_var.exp()- mu.pow(2))\n",
        "    return kld_loss\n",
        "\n",
        "def vae_loss(gen_images, input_images, mu_sigmas):\n",
        "    \"\"\"\n",
        "    Computes BCE reconstruction loss and KL div. loss for VAE outputs.\n",
        "\n",
        "    Input:\n",
        "    - gen_images:   list of 2 decoded image sequences, each of shape (seq_length,\n",
        "                    n_channels, img_width, img_height) which defaults to\n",
        "                    (50, 1, 64, 64). In the baseline model, this will contain\n",
        "                    one sequence decoded from the VAE itself, and another decoded\n",
        "                    from the top layer of the Transformer.\n",
        "    - input_images: list of target image sequences, each of shape (seq_length,\n",
        "                    n_channels, img_width, img_height) which defaults to\n",
        "                    (50, 1, 64, 64). The nth sequence in gen_images will be\n",
        "                    evaluated against the nth sequence in input_images to\n",
        "                    compute the reconstruction loss. In the baseline mode, this\n",
        "                    will contain one sequence so is purly a tensor.\n",
        "    - mu_sigmas:    list of (mu, sigma) tuples, where each mu and sigma is a\n",
        "                    sequence of shape (seq_length, VAE.z_dim). In the baseline\n",
        "                    model, this will contain one tuple from the VAE and another\n",
        "                    from the Transformer.\n",
        "\n",
        "    Returns:\n",
        "    - BCEs: a list containing the total BCE reconstruction loss for each image\n",
        "            sequence\n",
        "    - KLDs: a list containing the total KL divergence loss for each mu/sigma pair\n",
        "    \"\"\"\n",
        "\n",
        "    # List to aggregate binary cross-entropy reconstruction losses\n",
        "    # from all of the image outputs:\n",
        "    BCEs = []\n",
        "    # List to aggregate KL divergence losses from each of the mu/sigma\n",
        "    # projections:\n",
        "    KLDs = []\n",
        "\n",
        "    # TODO Your code goes here.\n",
        "    num_methods = len(gen_images)\n",
        "    for i in range(num_methods):\n",
        "\n",
        "        gen_seqs = gen_images[i]\n",
        "\n",
        "        for gen_seq, target_seq in zip(gen_seqs, input_images):\n",
        "            gen_seq = gen_seq.view(-1)\n",
        "            target_seq = target_seq.view(-1)\n",
        "\n",
        "            bce_loss = F.binary_cross_entropy(gen_seq, target_seq, reduction='mean')\n",
        "\n",
        "            BCEs.append(bce_loss)\n",
        "\n",
        "        mu, sigma = mu_sigmas[i]\n",
        "        mu = mu.view(-1)\n",
        "        sigma = sigma.view(-1)\n",
        "        sigma = torch.clamp(sigma,min=0.1)\n",
        "        log_var = 2 * torch.log(sigma)\n",
        "\n",
        "        kld_loss = kld(mu, log_var)\n",
        "\n",
        "        KLDs.append(kld_loss)\n",
        "\n",
        "\n",
        "    return BCEs, KLDs"
      ],
      "metadata": {
        "id": "qc_DJu8yb4Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMiUUm1Vb8mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5bIqMIjAb8ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XmEdTOVob8vF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q0fVVfCOb8xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contibutions:\n",
        "* aga149: I implemented a VAE and tried some experiments to get a better results. I did not got too much better results, but then, I continued to work on yoyo's branch to produce better results than before (0.6).\n",
        "*\n",
        "*"
      ],
      "metadata": {
        "id": "YuvWjOv6b8z4"
      }
    }
  ]
}