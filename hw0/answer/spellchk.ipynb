{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmBohaDi4Ow_"
   },
   "source": [
    "# spellchk idea 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "skHQ3xuk29O7"
   },
   "source": [
    "## Introduction\n",
    "### Modifications based on the differences in character sets\n",
    "\n",
    "*   We made changes to the code to make the predictions more accurate.\n",
    "*   We added a step to measure how different the predicted word is from the original mistake. This helps us understand the level of similarity or dissimilarity between the two.\n",
    "*   This is done by checking the differences in character sets in the typo and the predicted word.\n",
    "*   The idea here is that, for typo, most of the times, the typo word may have repeated the same character twice, or failed to type in such character, or the character is replaced by a different one.\n",
    "*   This means that the character sets used in the typo word should be highly similar to the one used in the correct word, meaning that the differences between the character set used in the typo word and correct word should be minimal.\n",
    "*   For example, for the word factor, the character set is C(factor) = {'a', 'c', 'f', 'o', 'r', 't'}, some more common typos may be facttor, this have the same character set as the correct word, as C(facttor) = {'a', 'c', 'f', 'o', 'r', 't'} = C(factor); facor, C(facor) = {'a', 'c', 'f', 'o', 'r'}, C(facor) - C(factor) = {}, C(factor) - C(facor) = {'t'}, the maximum differences between the 2 is the set of {'t'}, which is a difference of 1 character; facdor, C(facdor) = {'a', 'c', 'd', 'f', 'o', 'r'}, C(facdor) - C(factor) = {'d'}, C(factor) - C(facdor) = {'t'}, we can see how the maximum differences is 1 character in either cases.\n",
    "*   Based on this principle, we compared the differences in character sets, and generate the largest length differences between the 2 character sets.\n",
    "*   We used the code val[\"len_diff\"] = max(len(set(val[\"token_str\"]) - set(typo)), len(set(typo) - set(val[\"token_str\"]))) to calculate the maximum differences between the 2 data set.\n",
    "*   We can see that predict word character sets - typo word character sets, and typo word character sets - predict word character sets are used, the reason why both of them are needed is that, if a set is a subset of another set, then subset - the superset will have no objects in it, only the superset - subset will result in some objects, thus, we need to find the largest differences in length for the 2 cases. \n",
    "*   This is then used to select the best output in the line val[\"overall_score\"] = - val[\"len_diff\"], with the output having the lowest differences being the most likely to be the actual correct word. \n",
    "*   We can see this in action with an example, let's say we have a typo wear, with the correct word being war, since the character sets used for the typo is {'w', 'e', 'a', 'r'} and the correct word is {'w', 'a', 'r'}, the correct word has a difference of 1 to the typo. \n",
    "*   Assume that the model generated 3 words, being \".\", \"war\", \"pair\", with \".\" having the highest score, and \"pair\" having the lowest score, \".\" would be the output without this selection, but with the selection, the character set differences is 4 for this option, 1 for the option \"war\", and 2 for the option \"pair\", since \"war\" has the lowest differences, it will be selected as the output, which is the correct word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GlzZwu_24OxC"
   },
   "source": [
    "## Try1. Modify select_correction function\n",
    "\n",
    "__Idea:__ We compare the predict tokens with the typo, calculating the difference between the strings, then we sort the predict list based on the difference ascedingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3ui0W7j34OxB"
   },
   "outputs": [],
   "source": [
    "from spellchk import *\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TJ-w3D0i4OxC"
   },
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    for val in predict:\n",
    "        val[\"len_diff\"] = max(len(set(val[\"token_str\"]) - set(typo)), len(set(typo) - set(val[\"token_str\"])))\n",
    "        val[\"overall_score\"] = - val[\"len_diff\"]\n",
    "    new_predict = sorted(predict, key = lambda x: x[\"overall_score\"], reverse = True)\n",
    "    return new_predict[0][\"token_str\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5NY4R63U4OxC"
   },
   "source": [
    "Outcome: After improving our select_correction function, our dev score goes to 0.65"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## idea 1.1\n",
    "### Modification based on the differences in character sets and the differences of length of the tokens themselves\n",
    "\n",
    "*   We can see how, by using the differences in the character sets used, we are able to increase the accuracy massively, but what if we also integrate the absolute differences of length of the tokens?\n",
    "*   The idea is similar to the differences in character sets strategy, but we also added the comparison between the absolute length of the tokens themselves.\n",
    "*   This is for checking edge cases, where the model may have generated multiple outputs with the same differences in character set, but have large differences between the length of the characters themselves.\n",
    "*   Let's consider the typo word followerr, it has a length of 9, and the character set of {'e', 'f', 'l', 'o', 'r', 'w'}, the correct word is follower, which have length 8, with the same character set. Let's imagine that the model generated 2 words, \"follower\" and \"flower\", with flower rated higher, in the previous idea, both predictions will have the same character set, so flower may be chosen still, however, if we integrate raw length as well, \"follower\" have a length difference of 1, but flower have a length difference of 3, so follower will be chosen instead, which is the correct answer.\n",
    "*   We can see that the new score is based on both the differences in characters sets and token length differences, with abs(len(val[\"token_str\"]) - len(typo)) calculating the differences between the token length themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    for val in predict:\n",
    "        val[\"combined_len_diff\"] = max(len(set(val[\"token_str\"]) - set(typo)), len(set(typo) - set(val[\"token_str\"]))) + abs(len(val[\"token_str\"]) - len(typo))\n",
    "        val[\"overall_score\"] = - val[\"combined_len_diff\"]\n",
    "    predict = sorted(predict, key = lambda x: x[\"overall_score\"], reverse = True)\n",
    "    return predict[0]['token_str']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome: Despite adding the new checking, the accuracy remains the same at 0.65, the reason is likely because the edge case specified may not be very common, however, in a larger set of data with typos, it is likely that this will produce slightly better result than the first solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GPji2iaq4OxC"
   },
   "source": [
    "## Try2. Modify spellchk function\n",
    "\n",
    "__Idea:__ Now that we already make full use of the typo, it seems like the breakthrough can only happen in the prediction part. Through experience, we found that by increasing the prediction list, i.e., taking more consideration to the possible replacement, we can achieve a better score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspiration__: Flaw of small prediction Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surprised', 'happier', 'disappointed', 'happy', 'ashamed', 'satisfied', 'forgotten', 'punished', 'proud', 'disturbed', 'offended', 'lucky', 'harmed', 'shocked', 'confused', 'fooled', 'pleased', 'amazed', 'mistaken', 'embarrassed']\n",
      "fooled\n",
      "['beliefs', 'myths', 'ideas', 'linguistic', 'religions', 'languages', 'ethics', 'language', 'knowledge', 'theories', 'scientific', 'laws', 'traditions', 'anthropology', 'cultures', 'philosophy', 'morals', 'religion', 'philosophical', 'values']\n",
      "ideas\n",
      "['abilities', 'powers', 'spirits', 'dreams', 'memories', 'weaknesses', 'illusions', 'talents', 'pains', 'visions', 'minds', 'desires', 'stones', 'diseases', 'roots', 'bodies', 'patterns', 'souls', 'emotions', 'skills']\n",
      "emotions\n"
     ]
    }
   ],
   "source": [
    "str2 = \"7,14,16\\tSo I think we would not be live if our ancestors did not develop siences and tecnologies .\"\n",
    "\n",
    "with StringIO(str2) as f:\n",
    "    for (locations, sent) in get_typo_locations(f):\n",
    "        for i in locations:\n",
    "            predict = fill_mask(\n",
    "                    \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                    top_k=20\n",
    "                )\n",
    "            print([p[\"token_str\"] for p in predict])\n",
    "            print(select_correction(sent[i],predict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predict given is __\"So I think we would not be fooled if our ancestors did not develop ideas and emotions .\"__\n",
    "\n",
    "The predict list returned:\n",
    "\n",
    "- predict for 'live' : ['surprised', 'happier', 'disappointed', 'happy', 'ashamed', 'satisfied', 'forgotten', 'punished', 'proud', 'disturbed', 'offended', 'lucky', 'harmed', 'shocked', 'confused', 'fooled', 'pleased', 'amazed', 'mistaken', 'embarrassed']\n",
    "\n",
    "\n",
    "- predict for 'siences':['beliefs', 'myths', 'ideas', 'linguistic', 'religions', 'languages', 'ethics', 'language', 'knowledge', 'theories', 'scientific', 'laws', 'traditions', 'anthropology', 'cultures', 'philosophy', 'morals', 'religion', 'philosophical', 'values']\n",
    "\n",
    "\n",
    "- predict for 'tecnologies': ['abilities', 'powers', 'spirits', 'dreams', 'memories', 'weaknesses', 'illusions', 'talents', 'pains', 'visions', 'minds', 'desires', 'stones', 'diseases', 'roots', 'bodies', 'patterns', 'souls', 'emotions', 'skills']\n",
    "\n",
    "It seems like the correct word 'alive', 'science', 'technology' did not appear in the predict list. Thus, no matter how hard we try to improve the select_correction function, the correct answer will never be given. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Our Approaches to expland the predict list__: \n",
    "\n",
    "1. Increase the top_k value from 20 to 35\n",
    "\n",
    "2. Expand predict list by adding the predic using the truncated sentence\n",
    "\n",
    "Using method 1, we increase our dev score from 0.65 to 0.69, the optimal top_k value we found is 35. (original 20)\n",
    "\n",
    "Using method 2, we increase our dev score from 0.69 to 0.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FOEtofIX4OxE",
    "outputId": "29be2456-521d-4da6-9e62-7e9f61184c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\tSo I think we would not be alive\n",
      "7\tSo I think we would not be alive .\n",
      "7\tSo I think we would not be alive if our ancestors did not develop siences and tecnologies .\n"
     ]
    }
   ],
   "source": [
    "str2 = \"7,14,16\\tSo I think we would not be live if our ancestors did not develop siences and tecnologies .\"\n",
    "str_list = [str2]\n",
    "\n",
    "def spellchk(fh):\n",
    "    for (locations, sent) in get_typo_locations(fh):\n",
    "        spellchk_sent = sent\n",
    "        for i in locations:\n",
    "            predict = fill_mask(\n",
    "                \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), \n",
    "                top_k=35\n",
    "            )\n",
    "            if i < len(sent):\n",
    "                predict+=fill_mask(\n",
    "                    \" \".join([ sent[:i+1][j] if j != i else mask for j in range(len(sent[:i+1])) ]), \n",
    "                    top_k=35\n",
    "                )\n",
    "            spellchk_sent[i] = select_correction(sent[i], predict)\n",
    "        yield(locations, spellchk_sent)\n",
    "\n",
    "for str_test in str_list:\n",
    "    with StringIO(str_test) as f:\n",
    "        for (locations, spellchk_sent) in spellchk(f):\n",
    "            print(\"{locs}\\t{sent}\".format(\n",
    "                locs=\",\".join([str(i) for i in locations]),\n",
    "                sent=\" \".join(spellchk_sent)\n",
    "            ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eW2agQzd4OxF"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "The main idea behind the implemented changes was to enhance the accuracy of typo corrections. This was achieved by considering the dissimilarity between the predicted token and the typo, particularly in terms of character length. By calculating the length difference and introducing an overall scoring mechanism, the system was able to prioritize corrections that closely matched the intended correction. The scoring mechanism assigned higher scores to predictions with smaller length differences, indicating better matches. The sorting of predictions based on their scores ensured that the correction with the lowest score, indicating the best match, was chosen as the final correction. These modifications collectively improved the accuracy of the typo correction process by taking into account both the dissimilarity in length and the ranking of predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MKLxgjQE4OxF"
   },
   "source": [
    "## Group work\n",
    "\n",
    "* aga149: worked on the .ipynb report to explain clearly the changes in the spellchk.py. also reconduct the experience on the code to get the same results as well.\n",
    "* zwa204: Conduct raw experiment comparing the typo and the predict token; Expand the predict list to obtain better score.\n",
    "* thl28: Experimented with various techniques of getting better accuracy, including analysis of prediction and typo token, such as the differences in character set, length of the token itself etc, described approaches in .ipynb report.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
